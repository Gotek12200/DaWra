---
title: "Supervised learning competition"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(corrplot)
library(GGally)
library(janitor)
library(caTools)
library(car)
library(fastDummies)
library(kknn)
library(keras3)
```

```{r}
#| label: data loading
#| echo: false

train <- readRDS("train.rds")
test <- readRDS("test.rds")
```

# Data description

The dataset represents the characteristics of the secondary school students and additional information about their family background with their specific profile information, such as age, education of male or female partent etc.

It has a selection of categorical (factor) (such as school, sex, adress..) and continuous (numeric) quantitative variables such as age, as well as boolean yes or no variables. 

The split between males and females is relatively equal (Female: 173, Male: 143). The score range volatiles from -2.71 to 2.23, which shows that the variable is standardized, implying the formula (raw_score-mean/sd) was implied for better interpretation of students' results. 

Based on the graph below, majority of students have the score below the norm(-0.4 in our case), but most of scores are centered around +-1, showcasing normal average performance. Scores close to -3 to 2.5 are in minority and are considered outliers from students' norm. 

```{r}
ggplot(train, aes(x = score)) +
  geom_histogram(fill = "skyblue", color = "white", bins = 20) +
  theme_minimal() +
  labs(title = "Distribution of Student Scores",
       x = "Score", y = "Count")

# Summary statistics for numerical variables
train %>% 
  select(where(is.numeric)) %>% 
  summary()

```

Since we make prediction on score, in test set we do not include this variable and train the dataset based on the other characteristics to then try to predict for those test observations. 

```{r}
# Preview the data
glimpse(train)
```

```{r}
# Compute correlation matrix for numeric variables
num_vars <- train %>% select(where(is.numeric)) %>% select(-score)
cor_mat <- cor(num_vars, use = "pairwise.complete.obs")

corrplot(cor_mat, method = "color", type = "upper", tl.cex = 0.8)

# Correlation of each numeric variable with score
correlations <- train %>%
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  select(Variable, score) %>%
  arrange(desc(abs(score)))

head(correlations, 10)
```
# Data transformation and pre-processing

### Checking for & handling missing values
First, we check the dataset for missing values that could affect the result.
```{r}
train %>% summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_count") %>%
  arrange(desc(missing_count))
```
Fortunately, based on the output, we have no missing values in the dataset.

### Converting categorical variables to factors
Next, we need to convert categorical variables which are currently stored as text to factors to use for training our models.
```{r}
train_clean <- train %>%
  mutate(across(where(is.character), as.factor))
```

### Converting yes/no variables to boolean values
Similarly, yes or no values will be converted to boolean values.
```{r}
# Identify yes/no columns automatically and assign them boolean values
train_clean <- train_clean %>%
  mutate(across(
    where(~ (is.character(.) || is.factor(.)) && all(tolower(unique(.)) %in% c("yes", "no"))),
    ~ tolower(as.character(.)) == "yes"
  ))
```

### Normalizing numerical values
For k-nearest neighbors and neural nets it is essential to normalize numerical values to the same scale, otherwise variables with a different range of values would have a much bigger effect on the model than others. 

The numerical variables which are a score from 1-5 (or 1-4) are not continous are scaled using min-max scaling (because we can't assume normal distribution there).
```{r}
# scaling 1-5 scores (5 will be 1 and 1 will be scaled to 0)
train_norm <- train_clean %>% mutate(across(c("famrel","freetime","goout","Dalc","Walc", "health"), ~ (. - 1) / 4))

# scaling 1-4 scores (4 will be 1 and 1 will be scaled to 0)
train_norm <- train_clean %>% mutate(across(c(Medu, Fedu), ~ . / 4))
```

Some numerical variables are encoding bins (like `studytime` or `traveltime`). In this case, to keep the distances more preserved, we assign the mid-point of the bins and normalize those (for instance the value 2 in studytime means 2 to 5 hours of studying - in this case we replace 2 with 3.5 and normalize that).
```{r}
# scaling bin-type variables to their midpoints (traveltime and studytime)

# travel time in minutes with approximate midpoints
train_norm <- train_norm %>%
  mutate(traveltime = case_when(
    traveltime == 1 ~ 7.5,
    traveltime == 2 ~ 22.5,
    traveltime == 3 ~ 45,
    traveltime == 4 ~ 75
  )) 

# studytime in hours with approximate midpoints
train_norm <- train_norm %>%
  mutate(studytime = case_when(
    studytime == 1 ~ 1,
    studytime == 2 ~ 3.5,
    studytime == 3 ~ 7.5,
    studytime == 4 ~ 12
  ))

train_norm <- train_norm %>% 
  mutate(
    traveltime = (traveltime - min(traveltime)) / (max(traveltime) - min(traveltime)),
    studytime = (studytime - min(studytime)) / (max(studytime) - min(studytime))
    )
```

The continous variables like `failures`, `age` and `absences` are normalized using z-score normalization.
```{r}
# scaling using normal distribution
train_norm <- train_norm %>% mutate(across(c(failures, age, absences), ~ as.numeric(scale(.))))
```

### Check for outliers with the statistical approach
Next we check for outliers, to avoid the influence of some irregular datapoint on the models. We detect the outliers for continous variables by checking whether they are 3 standard deviations from the mean.The variables we check here are `age`, `failures` and `absences`.
```{r}
# detecting and counting outliers in continous numerical columns
z_scores <- train_norm[, c("age","absences","failures")]
outliers <- as.data.frame(abs(z_scores) > 3)
colSums(outliers)
```
As we can see, there are some outliers in the dataset. Now we can remove them and plot how the distribution changed using boxplots.
```{r}
#| warning: false

# replacing outliers with NA
train_wo_outliers <- train_norm %>% mutate(across(c("age", "failures", "absences"), ~ ifelse(outliers[[cur_column()]], NA, .)))

# creating data plot for plotting
before_after <- data.frame(
    age_before = z_scores$age,
    age_after = train_wo_outliers$age,
    absences_before = z_scores$absences,
    absences_after = train_wo_outliers$absences,
    failures_before = z_scores$failures,
    failures_after = train_wo_outliers$failures
)

# long format for boxplots
before_after <- before_after %>% pivot_longer(
  cols = everything(),
  names_to = c("variable", "time"),
  names_sep = "_",
  values_to = "value"
) %>% 
  mutate(time = factor(time, levels = c("before", "after")))

# Before after plot
ggplot(before_after, aes(x=variable, y=value, fill=time)) + 
  geom_boxplot()

# Removing NA rows from the dataset (actually removing the outliers)
cat("Rows containing outliers:", sum(!complete.cases(train_wo_outliers)), "\n")
train_wo_outliers <- na.omit(train_wo_outliers)
cat("Remaining rows:", nrow(train_wo_outliers), "\n")
```

### One-hot-encoding for categorical values
Deep learning can only handle numerical values, so we need to create a one-hot encoding for all categorical variables.
```{r}
nominal_vars <- c("Mjob","Fjob","reason","guardian")

train_one_hot <- dummy_cols(
  train_norm,
  select_columns = nominal_vars,
  remove_selected_columns = TRUE
)
```

We also need to convert binary factors to 0 or 1.
```{r}
binary_vars <- c("school","sex","address", "famsize", "Pstatus")

# assigning TRUE or FALSE to binary values (the second possible value is assigned 1 the first 0)
train_one_hot <- train_one_hot %>% mutate(across(all_of(binary_vars), ~ as.integer(.x == levels(.x)[2])))
```


### Train test split
We need to split the train dataset to a sample used for training, a sample used for validation during training and a test sample to check performance after the training is done. We have decided to split the train data 80:10:10 (training: validation: testing)
```{r}
# set seed for reproducibility
set.seed(123)

#first split the full data set in a 80/20 test train set then split the test val data 50/50 so they both become 10% of the full dataset
train_split <- sample.split(train$score, SplitRatio = 0.8)
val_split <- sample.split(train$score[!train_split], SplitRatio = 0.5)

# save the row indices that are part of the set
train_ids <- which(train_split)
temp_ids <- which(!train_split)
val_ids <- temp_ids[val_split]
test_ids <- temp_ids[!val_split]

# dataset for k-nearest neighbors and linear regression (normalized)
train_set <- train_norm[train_ids, ]
test_set <- train_norm[test_ids, ]
val_set <- train_norm[val_ids, ]

# dataset for deep learning (normalized & one-hot encoded)
train_deep <- train_one_hot[train_ids, ]
test_deep <- train_one_hot[test_ids, ]
val_deep <- train_one_hot[val_ids, ]
```
# Model description

## Linear regression
```{r}
#Creating the model
lin_regression <- lm(score ~ ., data = train_set)

#Check the performance of the model
summary(lin_regression)
plot(lin_regression)

#calculate the MSE for the linear model
predictions <- predict(lin_regression, newdata = test_set)
true_score <- test_set$score

lin_reg_mse <- mean((true_score- predictions)^2)

print(lin_reg_mse)
```
The fitted model explains approximately 35.5% of the variance in student scores, indicating that at least one of the predictors contributes to explaining variation in the dependent variable.
### Assumption Checking

1. Residuals vs Fitted

The residuals appear randomly scattered around zero with no clear pattern, this means that the linearity assumption is met. The slight curvature in the red trend line is minor and does not indicate strong nonlinearity.

2. Q–Q Plot

The residuals fall closely along the 45° reference line, implying that the normality assumption of residuals is reasonably satisfied. A few mild deviations at the tails are visible but not severe enough to threaten inference validity.

3. Scale–Location Plot

The spread of residuals is roughly constant across fitted values, supporting the homoscedasticity assumption (equal variance of errors).There is no strong funnel shape,

4. Residuals vs Leverage

Most data points fall within the Cook’s distance boundaries, suggesting no highly influential observations that unduly affect model estimates. A few points  show slightly higher leverage but remain within acceptable limits.

### Checking for Multicollinearity

To assess multicollinearity among the independent variables, the Variance Inflation Factor (VIF) was computed using the `car::vif()` function. The Generalized VIF (GVIF) and its adjusted value `GVIF^(1/(2*Df))` were examined to identify any predictors exhibiting high multicollinearity.

| Threshold | Interpretation |
|------------|----------------|
| < 2        | Low multicollinearity (ideal) |
| 2–5        | Moderate multicollinearity (acceptable) |
| > 5        | High multicollinearity (problematic) |

```{r}
# Check VIF values
vif(lin_regression)
```

```{r}
vif_values <- vif(lin_regression)
high_vif <- vif_values[vif_values[, "GVIF"] > 5, ]
high_vif
```

### Rebuilding the model
```{r}
vars_to_remove <- c( "Mjob", "Fjob", "reason", "guardian")

train_data_vif <- train_linear[, !(names(train_linear) %in% vars_to_remove)]
test_data_vif <- test_linear[, !(names(test_linear) %in% vars_to_remove)]

#Fit linear regression without those variables
lin_regression_vif <- lm(score ~ ., data = train_data_vif)

#Summary of the model
summary(lin_regression_vif)
plot(lin_regression_vif)

#Predict on test data
predictions_vif <- predict(lin_regression_vif, newdata = test_data_vif)
true_score <- test_data_vif$score

#Compute Mean Squared Error
lin_reg_mse_vif <- mean((true_score - predictions_vif)^2)

cat("Linear Regression MSE (after removing high-VIF variables):",
round(lin_reg_mse_vif, 4), "\n")
```

## KNN-regression
The KNN regression is a non-parametric model which makes predictions based on what the closest neighbours are doing. Usually we pick K=5 and K=10, where K is the number of neighors considered. As a result, if we want to predict the score with the KNN regression model, we need to find the K most similar students and then take the average of the scores. The model we used will check all the possible K values with the maximum K equal to 10, and then it will return the predictions with the best K. For our data the best K is 10.

```{r}
#KNN regression without the outliers
model_1 <- train.kknn(
  score ~ .,
  data = train_set,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_1

plot(model_1)

pred <- predict(model_1, newdata = test_set)


#KNN regression with the outliers
model_2 <- train.kknn(
  score ~ .,
  data = train_wo_outliers,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_2

plot(model_2)


pred_wo_outliers <- predict(model_2, newdata = test)

#Check the impact of outliers
differences <- pred - pred_wo_outliers
summary(differences)
```

## Deep learning model
```{r}
# defining input and output
x_train <- train_deep %>% select(-score) %>% as.matrix()
x_val <- val_deep %>% select(-score) %>% as.matrix()
x_test <- test_deep %>% select(-score) %>% as.matrix()

# normalizing scores with z-score normalization 
y_train <- train_deep$score
y_val <- val_deep$score
y_test <- test_deep$score

# defining model
model <- keras_model_sequential() %>%
  layer_dense(units=32, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)

# compiling
model %>% compile(
  optimizer = "adam",
  loss = "mse",
  metrics = list("mae","mse")
)

# training
history <- model %>% fit(
  x = x_train,
  y = y_train,
  epochs = 20,
  validation_data = list(x_val, y_val),
  verbose = 0
)

# evaluation
model %>% evaluate(x_test, y_test, verbose = 1)
predictions <- model %>% predict(x_test)

df <- data.frame(index = 1:length(y_test), actual=y_test, predicted=as.vector(predictions))

ggplot(df, aes(x=index)) + geom_point(aes(y=actual, color="Actual")) + geom_point((aes(y = predicted, color = "Predicted")))
```


# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```




# Team member contributions

Write down what each team member contributed to the project.
- Mate Csikos-Nagy: set up template & git in R studio, deep learning model, model comparisons
- Author Two: b, c, d
- Author Three: a, b, d
