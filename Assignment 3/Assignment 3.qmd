---
title: "Supervised learning competition"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(corrplot)
library(GGally)
library(janitor)
library(caTools)
library(car)
```

```{r}
#| label: data loading
#| echo: false

train <- readRDS("train.rds")
test <- readRDS("test.rds")
```

# Data description

The dataset represents the characteristics of the secondary school students and additional information about their family background with their specific profile information, such as age, education of male or female partent etc.

It has a selection of categorical (factor) (such as school, sex, adress..) and continuous (numeric) quantitative variables such as age, male education, internet etc, the mix of which is well suited for regression or classification tasks. 

The split between males and females is relatively equal (Female: 173, Male: 143). The score range volatiles from -2.71 to 2.23, which shows that the variable is standardized, implying the formula (raw_score-mean/sd) was implied for better interpretation of students' results. 

Based on the graph below, majority of students have the score below the norm(-0.4 in our case), but most of scores are centered around +-1, showcasing normal average performance. Scores close to -3 to 2.5 are in minority and are considered outliers from students' norm. 

```{r}
ggplot(train, aes(x = score)) +
  geom_histogram(fill = "skyblue", color = "white", bins = 20) +
  theme_minimal() +
  labs(title = "Distribution of Student Scores",
       x = "Score", y = "Count")

# Summary statistics for numerical variables
train %>% 
  select(where(is.numeric)) %>% 
  summary()

```

Since we make prediction on score, in test set we do not include this variable and train the dataset based on the other characteristics to then try to predict for those test observations. 

```{r}
# Preview the data
glimpse(train)
```

```{r}
# Compute correlation matrix for numeric variables
num_vars <- train %>% select(where(is.numeric)) %>% select(-score)
cor_mat <- cor(num_vars, use = "pairwise.complete.obs")

corrplot(cor_mat, method = "color", type = "upper", tl.cex = 0.8)

# Correlation of each numeric variable with score
correlations <- train %>%
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  select(Variable, score) %>%
  arrange(desc(abs(score)))

head(correlations, 10)
```
# Data transformation and pre-processing

### Checking for & handling missing values
First, we check the dataset for missing values that could affect the result.
```{r}
train %>% summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_count") %>%
  arrange(desc(missing_count))
```
Fortunately, based on the output, we have no missing values in the dataset.

### Converting categorical variables to factors
Next, we need to convert categorical variables which are currently stored as text to factors to use for training our models.
```{r}
train_clean <- train %>%
  mutate(across(where(is.character), as.factor))
```

### Converting yes/no variables to boolean values
Similarly, yes or no values will be converted to 0 or 1.
```{r}
train_clean <- train_clean %>% 
  mutate(across(c("schoolsup","famsup","paid","activities","nursery","higher", "internet","romantic"), ~ ifelse(. == "yes", 1, 0)))
```{r}
# Identify yes/no columns automatically
yes_no_cols <- names(train_clean)[sapply(train_clean, function(x) {
  is.character(x) || is.factor(x)
}) & sapply(train_clean, function(x) {
  all(unique(na.omit(tolower(x))) %in% c("yes", "no"))
})]

train_clean <- train_clean %>%
  mutate(across(all_of(yes_no_cols),
                ~ tolower(as.character(.)) == "yes"))

```

### Normalizing numerical values
For k-nearest neighbors and neural nets it is essential to normalize numerical values to the same scale, otherwise variables with a different range of values would have a much bigger effect on the model than others. 


```{r}
num_cols <- names(train_clean)[sapply(train_clean, is.numeric)]
num_cols <- setdiff(num_cols, "score")

#Compute mean and sd for each numeric column



train_means <- sapply(train_clean[, num_cols, drop = FALSE], mean, na.rm = TRUE)
train_sds <- sapply(train_clean[, num_cols, drop = FALSE], sd, na.rm = TRUE)

#Avoid division by zero (if a column has zero variance)



train_sds[train_sds == 0 | is.na(train_sds)] <- 1

#Apply normalization (z-score scaling)

train_clean[, num_cols] <- sweep(
sweep(train_clean[, num_cols, drop = FALSE], 2, train_means, FUN = "-"),
2, train_sds, FUN = "/"
)

#Quick check, means should be ~0, sds ~1



summary(train_clean[, num_cols])

```

### Check for outliers with the statistical approach
```{r}
numeric_data <- train_clean[sapply(train_clean, is.numeric)]
z_scores <- scale(numeric_data)

head(z_scores)

outliers <- abs(z_scores) > 3
head(outliers)

train_cleaned <- train_clean[!apply(outliers, 1, any), ]

# Plot with outliers highlighted
numeric_cols <- sapply(train_clean, is.numeric)
boxplot(train_clean[, numeric_cols],
        main = "Boxplots of Numeric Variables",
        las = 2)

numeric_cols




```

```{r}
colSums(abs(z_scores) > 3)
numeric_data <- train_clean[sapply(train_clean, is.numeric)]
num_cols <- setdiff(names(numeric_data), "score")

# Compute z-scores for all numeric variables



z_scores <- scale(numeric_data)

# Identify rows with any z-score beyond ±3



outlier_rows <- apply(abs(z_scores[, num_cols]) > 3, 1, any)

# Remove those rows from the dataset



train_clean_no_outliers <- train_clean[!outlier_rows, ]
# Check how many were removed



cat("Removed", sum(outlier_rows), "rows due to |z| > 3 outliers.\n")
cat("Remaining rows:", nrow(train_clean_no_outliers), "\n")
```

### Train test split
We need to split the train dataset to a sample used for training, a sample used for validation during training and a test sample to check performance after the training is done. We have decided to split the train data 80:10:10 (training: validation: testing)
```{r}
# set seed for reproducibility
set.seed(123)

#first split the full data set in a 80/20 test train set
split1 <- sample.split(train_cleaned$score, SplitRatio = 80)
train_data <- subset(train_cleaned, split1 == TRUE)
test_val_data <- subset(train_cleaned, split1 == FALSE)

#then split the test val data 50/50 so they both become 10% of the full dataset
split2 <- sample.split(test_val_data$score, SplitRatio = 50)
test_data <- subset(test_val_data, split2 == TRUE)
validation_data <- subset(test_val_data, split2 == FALSE)
```
# Model description

### Linear regression

```{r}

lin_regression <- lm(score ~ ., data = train_data)

#Check the performance of the model
summary(lin_regression)
plot(lin_regression)

#calculate the MSE for the linear model
predictions <- predict(lin_regression, newdata = test_data)
true_score <- test_data$score

lin_reg_mse <- mean((true_score- predictions)^2)

print(lin_reg_mse)
```
### KNN-regression
The KNN regression is a non-parametric model which makes predictions based on what the closest neighbours are doing. Usually we pick K=5 and K=10, where K is the number of neighors considered. As a result, if we want to predict the score with the KNN regression model, we need to find the K most similar students and then take the average of the scores. The model we used will check all the possible K values with the maximum K equal to 10, and then it will return the predictions with the best K. For our data the best K is 10.

```{r}
#KNN regression without the outliers
library(kknn)

model_1 <- train.kknn(
  score ~ .,
  data = train_clean,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_1

plot(model_1)

pred <- predict(model_1, newdata =test)


#KNN regression with the outliers

model_2 <- train.kknn(
  score ~ .,
  data = train_cleaned,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_2

plot(model_2)


pred_with_outliers <- predict(model_2, newdata =test)


#Check the impact of outliers
differences <- pred - pred_with_outliers
summary(differences)
```

### Deep learning model

# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```
# Checking for Multicollinearity

To assess multicollinearity among the independent variables, the Variance Inflation Factor (VIF) was computed using the `car::vif()` function. The Generalized VIF (GVIF) and its adjusted value `GVIF^(1/(2*Df))` were examined to identify any predictors exhibiting high multicollinearity.

| Threshold | Interpretation |
|------------|----------------|
| < 2        | Low multicollinearity (ideal) |
| 2–5        | Moderate multicollinearity (acceptable) |
| > 5        | High multicollinearity (problematic) |

```{r}
# Check VIF values
vif(lin_regression)

```

```{r}
vif_values <- vif(lin_regression)
high_vif <- vif_values[vif_values[, "GVIF"] > 5, ]
high_vif


```

#rebuilding the model
```{r}
vars_to_remove <- c( "Mjob", "Fjob", "reason", "guardian")

train_data_vif <- train_data[, !(names(train_data) %in% vars_to_remove)]
test_data_vif <- test_data[, !(names(test_data) %in% vars_to_remove)]

#Fit linear regression without those variables

lin_regression_vif <- lm(score ~ ., data = train_data_vif)

#Summary of the model

summary(lin_regression_vif)
plot(lin_regression_vif)

#Predict on test data

predictions_vif <- predict(lin_regression_vif, newdata = test_data_vif)
true_score <- test_data_vif$score

#Compute Mean Squared Error

lin_reg_mse_vif <- mean((true_score - predictions_vif)^2)




cat("Linear Regression MSE (after removing high-VIF variables):",
round(lin_reg_mse_vif, 4), "\n")
```


# Team member contributions

Write down what each team member contributed to the project.
- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
