---
title: "Supervised learning competition"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(corrplot)
library(GGally)
library(janitor)
library(caTools)
```

```{r}
#| label: data loading
#| echo: false

train <- readRDS("train.rds")
test <- readRDS("test.rds")
```

# Data description

Describe the data and use a visualization to support your story. (approx. one or two paragraphs)

```{r}
# Preview the data
glimpse(train)
summary(train)

```

```{r}
ggplot(train, aes(x = score)) +
  geom_histogram(fill = "skyblue", color = "white", bins = 20) +
  theme_minimal() +
  labs(title = "Distribution of Student Scores",
       x = "Score", y = "Count")

# Summary statistics for numerical variables
train %>% 
  select(where(is.numeric)) %>% 
  summary()

```

```{r}

# Compute correlation matrix for numeric variables
num_vars <- train %>% select(where(is.numeric)) %>% select(-score)
cor_mat <- cor(num_vars, use = "pairwise.complete.obs")

corrplot(cor_mat, method = "color", type = "upper", tl.cex = 0.8)

# Correlation of each numeric variable with score
correlations <- train %>%
  select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  select(Variable, score) %>%
  arrange(desc(abs(score)))

head(correlations, 10)
```
# Data transformation and pre-processing

### Checking for & handling missing values
```{r}
train %>% summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_count") %>%
  arrange(desc(missing_count))
```

```{r}
train_clean <- train %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```

### Converting categorical variables to factors
```{r}
train_clean <- train_clean %>%
  mutate(across(where(is.character), as.factor))
```

### Converting yes/no variables to boolean values

### Normalizing numerical values

### Check for outliers with the statistical approach
```{r}
numeric_data <- train_clean[sapply(train_clean, is.numeric)]
z_scores <- scale(numeric_data)

head(z_scores)

outliers <- abs(z_scores) > 3
head(outliers)

train_cleaned <- train_clean[!apply(outliers, 1, any), ]

# Plot with outliers highlighted
numeric_cols <- sapply(train_clean, is.numeric)
boxplot(train_clean[, numeric_cols],
        main = "Boxplots of Numeric Variables",
        las = 2)

numeric_cols
```
# Train test split
```{r}
set.seed(123)

#first split the full data set in a 80/20 test train set
split1 <- sample.split(train_cleaned$score, SplitRatio = 80)
train_data <- subset(train_cleaned, split1 == TRUE)
test_val_data <- subset(train_cleaned, split1 == False)

#then split the test val data 50/50 so they both become 10% of the full dataset
split2 <- sample.split(test_val_data$score, SplitRatio = 50)
test_data <- subset(test_val_data, split2 == TRUE)
validation_data <- subset(test_val_data, split2 == False)
```
# Model description

### Linear regression

```{r}

lin_regression <- lm(score ~ ., data = train_data)

#Check the performance of the model
summary(lin_regression)
plot(lin_regression)

#calculate the MSE for the linear model
predictions <- predict(lin_regression, newdata = test_data)
true_score <- test_data$score

lin_reg_mse <- mean((true_score- predictions)^2)

print(lin_reg_mse)
```
### KNN-regression
##The KNN regression is a non-parametric model which makes predictions based on what the closest neighbours are doing. Usually we pick K=5 and K=10, where K is the number of neighors considered. As a result, if we want to predict the score with the KNN regression model, we need to find the K most similar students and then take the average of the scores. The model we used will check all the possible K values with the maximum K equal to 10, and then it will return the predictions with the best K. For our data the best K is 10.

```{r}
#KNN regression without the outliers
library(kknn)

model_1 <- train.kknn(
  score ~ .,
  data = train_clean,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_1

plot(model_1)

pred <- predict(model_1, newdata =test)


#KNN regression with the outliers

model_2 <- train.kknn(
  score ~ .,
  data = train_cleaned,
  kmax = 10,
  distance = 2,
  kernel = "optimal"
)

model_2

plot(model_2)


pred_with_outliers <- predict(model_2, newdata =test)


#Check the impact of outliers
differences <- pred - pred_with_outliers
summary(differences)
```

### Deep learning model

# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d