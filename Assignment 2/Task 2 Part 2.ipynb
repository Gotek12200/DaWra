{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29210e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e52786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACM = pd.read_csv(\"ACM.csv\")\n",
    "df_DB = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0b5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Concatenate all columns per row into one string\n",
    "df_DB[\"merged\"] = df_DB.astype(str).agg(\" \".join, axis=1)\n",
    "df_ACM[\"merged\"] = df_ACM.astype(str).agg(\" \".join, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4686722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "df_DB[\"merged\"] = df_DB[\"merged\"].str.lower()\n",
    "df_ACM[\"merged\"] = df_ACM[\"merged\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1dd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "df_DB[\"merged\"] = df_DB[\"merged\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df_ACM[\"merged\"] = df_ACM[\"merged\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616 records combined.\n",
      "0    304586 the wasa2 object-oriented workflow mana...\n",
      "1    304587 a user-centered interface for querying ...\n",
      "2    304589 world wide database-integrating the web...\n",
      "3    304590 xml-based information mediation with mi...\n",
      "4    304582 the ccube constraint object-oriented da...\n",
      "Name: merged, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "list_ACM = df_ACM[\"merged\"]\n",
    "list_DB  = df_DB[\"merged\"]\n",
    "\n",
    "big_list = list_ACM + list_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db523069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 vs Doc 1\n",
      "  Jaccard (3-shingles): 0.1763\n",
      "  MinHash estimate    : 0.2100\n",
      "\n",
      "Pairwise similarities (first N docs):\n",
      "    doc_a  doc_b   jaccard  minhash\n",
      "0       0      1  0.176334     0.15\n",
      "1       0      2  0.181818     0.18\n",
      "2       0      3  0.169388     0.16\n",
      "3       0      4  0.220588     0.22\n",
      "4       0      5  0.171657     0.23\n",
      "5       0      6  0.237164     0.28\n",
      "6       0      7  0.197436     0.22\n",
      "7       0      8  0.143868     0.15\n",
      "8       0      9  0.170792     0.20\n",
      "9       1      2  0.218029     0.17\n",
      "10      1      3  0.220126     0.19\n",
      "11      1      4  0.204276     0.22\n",
      "12      1      5  0.148362     0.13\n",
      "13      1      6  0.181193     0.14\n",
      "14      1      7  0.192982     0.16\n",
      "15      1      8  0.100223     0.11\n",
      "16      1      9  0.105505     0.09\n",
      "17      2      3  0.191529     0.16\n",
      "18      2      4  0.209302     0.21\n",
      "19      2      5  0.159649     0.14\n"
     ]
    }
   ],
   "source": [
    "#5 Shingle used from the lap\n",
    "def shingle(text, k: int) -> set:\n",
    "    \"\"\"Return set of k-shingles, robust to None/NaN/non-strings/short strings.\"\"\"\n",
    "    # Treat None/NaN as empty\n",
    "    if text is None:\n",
    "        return set()\n",
    "    if isinstance(text, float):\n",
    "        if text != text:  # NaN\n",
    "            return set()\n",
    "        text = str(text)\n",
    "    elif not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    if len(text) < k:\n",
    "        return set()\n",
    "    return { text[i:i+k] for i in range(len(text) - k + 1) }\n",
    "\n",
    "def build_vocab(shingle_sets: list) -> dict:\n",
    "    full_set = {sh for s in shingle_sets for sh in s}\n",
    "    return {sh: i for i, sh in enumerate(full_set)}\n",
    "\n",
    "def one_hot(shingles: set, vocab: dict):\n",
    "    vec = np.zeros(len(vocab), dtype=int)\n",
    "    for sh in shingles:\n",
    "        vec[vocab[sh]] = 1\n",
    "    return vec\n",
    "\n",
    "def get_minhash_arr(num_hashes:int, vocab:dict):\n",
    "    length = len(vocab)\n",
    "    arr = np.zeros((num_hashes, length), dtype=int)\n",
    "    for i in range(num_hashes):\n",
    "        arr[i, :] = np.random.permutation(length) + 1\n",
    "    return arr\n",
    "\n",
    "def get_signature(minhash: np.ndarray, vector: np.ndarray):\n",
    "    idx = np.nonzero(vector)[0]\n",
    "    if idx.size == 0:\n",
    "        # No shingles; return a signature that won't match others\n",
    "        return np.full(minhash.shape[0], np.iinfo(np.int32).max, dtype=int)\n",
    "    return np.min(minhash[:, idx], axis=1)\n",
    "\n",
    "def jaccard_similarity(set1: set, set2: set) -> float:\n",
    "    inter = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def compute_signature_similarity(sig1: np.ndarray, sig2: np.ndarray) -> float:\n",
    "    if sig1.shape != sig2.shape:\n",
    "        raise ValueError(\"Signature shapes must match.\")\n",
    "    return float(np.mean(sig1 == sig2))\n",
    "\n",
    "# Shingling\n",
    "k = 3\n",
    "shingle_sets = [shingle(doc, k) for doc in big_list]\n",
    "\n",
    "# Vocab & one-hot\n",
    "vocab = build_vocab(shingle_sets)\n",
    "if len(vocab) == 0:\n",
    "    raise ValueError(\"Vocabulary is empty. Check that big_list has strings of length >= k.\")\n",
    "\n",
    "onehot = np.stack([one_hot(sset, vocab) for sset in shingle_sets])\n",
    "\n",
    "# MinHash signatures\n",
    "num_hashes = 100\n",
    "minhash_arr = get_minhash_arr(num_hashes, vocab)\n",
    "signatures = np.stack([get_signature(minhash_arr, vec) for vec in onehot])\n",
    "\n",
    "#  similarities\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
