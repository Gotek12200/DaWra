{
 "cells": [
  {
   "cell_type": "code",
   "id": "29210e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:09:45.948810Z",
     "start_time": "2025-10-06T21:09:45.940809Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "id": "82e52786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:16:30.397054Z",
     "start_time": "2025-10-06T19:16:30.368614Z"
    }
   },
   "source": [
    "df_ACM = pd.read_csv(\"ACM.csv\")\n",
    "df_DB = pd.read_csv(\"DBLP2.csv\", encoding=\"latin1\")"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "2a0b5ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:16:30.424921Z",
     "start_time": "2025-10-06T19:16:30.403700Z"
    }
   },
   "source": [
    "# 1. Concatenate all columns per row into one string\n",
    "df_DB[\"merged\"] = df_DB.astype(str).agg(\" \".join, axis=1)\n",
    "df_ACM[\"merged\"] = df_ACM.astype(str).agg(\" \".join, axis =1)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "4686722e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:16:30.437256Z",
     "start_time": "2025-10-06T19:16:30.431945Z"
    }
   },
   "source": [
    "# 2. Change all alphabetical characters into lowercase.\n",
    "df_DB[\"merged\"] = df_DB[\"merged\"].str.lower()\n",
    "df_ACM[\"merged\"] = df_ACM[\"merged\"].str.lower()"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "6d1dd9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T19:16:30.469751Z",
     "start_time": "2025-10-06T19:16:30.443754Z"
    }
   },
   "source": [
    "# 3. Convert multiple spaces to one.\n",
    "df_DB[\"merged\"] = df_DB[\"merged\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df_ACM[\"merged\"] = df_ACM[\"merged\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "0aa4d147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:00:23.108946Z",
     "start_time": "2025-10-06T20:00:23.095856Z"
    }
   },
   "source": [
    "# 4. Combine the records from both tables into one big list as we did during the lab.\n",
    "list_ACM = df_ACM[\"merged\"].tolist()\n",
    "list_DB  = df_DB[\"merged\"].tolist()\n",
    "\n",
    "big_list = list_ACM + list_DB\n",
    "print(f\"Total length of big_list: {len(big_list)}\")  # Should be 4910"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of big_list: 4910\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "id": "db523069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:33:28.736374Z",
     "start_time": "2025-10-06T20:31:02.404778Z"
    }
   },
   "source": [
    "# 5. Use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity.\n",
    "def shingle(text, k: int) -> set:\n",
    "    \"\"\"Return set of k-shingles, robust to None/NaN/non-strings/short strings.\"\"\"\n",
    "    # Treat None/NaN as empty\n",
    "    if text is None:\n",
    "        return set()\n",
    "    if isinstance(text, float):\n",
    "        if text != text:  # NaN\n",
    "            return set()\n",
    "        text = str(text)\n",
    "    elif not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    if len(text) < k:\n",
    "        return set()\n",
    "    return { text[i:i+k] for i in range(len(text) - k + 1) }\n",
    "\n",
    "def build_vocab(shingle_sets: list) -> dict:\n",
    "    full_set = {sh for s in shingle_sets for sh in s}\n",
    "    return {sh: i for i, sh in enumerate(full_set)}\n",
    "\n",
    "def one_hot(shingles: set, vocab: dict):\n",
    "    vec = np.zeros(len(vocab), dtype=int)\n",
    "    for sh in shingles:\n",
    "        vec[vocab[sh]] = 1\n",
    "    return vec\n",
    "\n",
    "def get_minhash_arr(num_hashes:int, vocab:dict):\n",
    "    length = len(vocab)\n",
    "    arr = np.zeros((num_hashes, length), dtype=int)\n",
    "    for i in range(num_hashes):\n",
    "        arr[i, :] = np.random.permutation(length) + 1\n",
    "    return arr\n",
    "\n",
    "def get_signature(minhash: np.ndarray, vector: np.ndarray):\n",
    "    idx = np.nonzero(vector)[0]\n",
    "    if idx.size == 0:\n",
    "        # No shingles; return a signature that won't match others\n",
    "        return np.full(minhash.shape[0], np.iinfo(np.int32).max, dtype=int)\n",
    "    return np.min(minhash[:, idx], axis=1)\n",
    "\n",
    "def jaccard_similarity(set1: set, set2: set) -> float:\n",
    "    inter = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def compute_signature_similarity(sig1: np.ndarray, sig2: np.ndarray) -> float:\n",
    "    if sig1.shape != sig2.shape:\n",
    "        raise ValueError(\"Signature shapes must match.\")\n",
    "    return float(np.mean(sig1 == sig2))\n",
    "\n",
    "# Shingling\n",
    "k = 3\n",
    "shingle_sets = [shingle(doc, k) for doc in big_list]\n",
    "\n",
    "# Vocab & one-hot\n",
    "vocab = build_vocab(shingle_sets)\n",
    "if len(vocab) == 0:\n",
    "    raise ValueError(\"Vocabulary is empty. Check that big_list has strings of length >= k.\")\n",
    "\n",
    "onehot = np.stack([one_hot(sset, vocab) for sset in shingle_sets])\n",
    "\n",
    "# MinHash signatures\n",
    "num_hashes = 100\n",
    "minhash_arr = get_minhash_arr(num_hashes, vocab)\n",
    "signatures = np.stack([get_signature(minhash_arr, vec) for vec in onehot])\n",
    "\n",
    "#  similarities\n",
    "N = len(big_list)\n",
    "jac_mat = np.eye(N, dtype=float) # exact Jaccard similarity matrix\n",
    "mh_mat  = np.eye(N, dtype=float)  # MinHash similarity matrix\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(i + 1, N):\n",
    "        # Exact Jaccard on shingles\n",
    "        s_jac = jaccard_similarity(shingle_sets[i], shingle_sets[j])\n",
    "        jac_mat[i, j] = jac_mat[j, i] = s_jac\n",
    "\n",
    "        # MinHash-based similarity (fraction of equal signature components)\n",
    "        s_mh = compute_signature_similarity(signatures[i], signatures[j])\n",
    "\n",
    "\n",
    "        mh_mat[i, j] = mh_mat[j, i] = s_mh\n"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "70af7f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T20:30:34.757524Z",
     "start_time": "2025-10-06T20:30:34.519358Z"
    }
   },
   "source": [
    "# print shingles\n",
    "print(\"\\n=== Example shingles (first 3 documents) ===\")\n",
    "for i, s in enumerate(shingle_sets[:3]):\n",
    "    print(f\"Doc {i}: {sorted(list(s))[:10]} ... ({len(s)} total shingles)\")\n",
    "\n",
    "# Print MinHash signatures\n",
    "print(\"\\n=== Example MinHash signatures (first 3 documents) ===\")\n",
    "for i, sig in enumerate(signatures[:3]):\n",
    "    print(f\"Doc {i} signature: {sig[:10]} ... ({len(sig)} total hash values)\")\n",
    "\n",
    "# Print Jaccard & MinHash similarities\n",
    "print(\"\\n=== Jaccard similarity matrix (rounded) ===\")\n",
    "print(np.round(jac_mat, 3))\n",
    "\n",
    "print(\"\\n=== MinHash similarity matrix (rounded) ===\")\n",
    "print(np.round(mh_mat, 3))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example shingles (first 3 documents) ===\n",
      "Doc 0: [' 19', ' co', ' da', ' go', ' in', ' ma', ' ob', ' of', ' on', ' sy'] ... (126 total shingles)\n",
      "Doc 1: [' 19', ' a ', ' co', ' cr', ' da', ' di', ' f.', ' fo', ' in', ' is'] ... (147 total shingles)\n",
      "Doc 2: [' 19', ' an', ' at', ' be', ' bo', ' co', ' da', ' he', ' in', ' ja'] ... (188 total shingles)\n",
      "\n",
      "=== Example MinHash signatures (first 3 documents) ===\n",
      "Doc 0 signature: [ 18 183  46  11 268  23  13 149  28  86] ... (140 total hash values)\n",
      "Doc 1 signature: [ 18 358  46  11  12  23 172 225 141  67] ... (140 total hash values)\n",
      "Doc 2 signature: [ 18  27  46  11  12  15 142 112  98 178] ... (140 total hash values)\n",
      "\n",
      "=== Jaccard similarity matrix (rounded) ===\n",
      "[[1.    0.252 0.256 ... 0.054 0.016 0.046]\n",
      " [0.252 1.    0.255 ... 0.098 0.019 0.083]\n",
      " [0.256 0.255 1.    ... 0.093 0.032 0.074]\n",
      " ...\n",
      " [0.054 0.098 0.093 ... 1.    0.071 0.082]\n",
      " [0.016 0.019 0.032 ... 0.071 1.    0.037]\n",
      " [0.046 0.083 0.074 ... 0.082 0.037 1.   ]]\n",
      "\n",
      "=== MinHash similarity matrix (rounded) ===\n",
      "[[1.    0.314 0.25  ... 0.079 0.021 0.021]\n",
      " [0.314 1.    0.329 ... 0.136 0.036 0.093]\n",
      " [0.25  0.329 1.    ... 0.114 0.064 0.079]\n",
      " ...\n",
      " [0.079 0.136 0.114 ... 1.    0.086 0.093]\n",
      " [0.021 0.036 0.064 ... 0.086 1.    0.014]\n",
      " [0.021 0.093 0.079 ... 0.093 0.014 1.   ]]\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T21:55:55.980983Z",
     "start_time": "2025-10-06T21:55:53.420788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" 6. Extract the top 2224 candidates from the LSH algorithm, compare them to the actual\n",
    "    mappings in the file DBLP-ACM_perfectMapping.csv\n",
    "    and compute the precision of the method.\n",
    "    7. Record the running time of the method.\n",
    "    8. Compare the precision and the running time in Parts 1 and 2.\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "perfect_mapping_df = pd.read_csv(\"DBLP-ACM_perfectMapping.csv\")\n",
    "# the following code is taken from lab 5\n",
    "class LSH:\n",
    "    \"\"\"\n",
    "    Implements the Locality Sensitive Hashing (LSH) technique for approximate\n",
    "    nearest neighbor search.\n",
    "    \"\"\"\n",
    "    buckets = []\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, b: int):\n",
    "        \"\"\"\n",
    "        Initializes the LSH instance with a specified number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - b (int): The number of bands to divide the signature into.\n",
    "        \"\"\"\n",
    "        self.b = b\n",
    "        for i in range(b):\n",
    "            self.buckets.append({})\n",
    "\n",
    "    def make_subvecs(self, signature: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Divides a given signature into subvectors based on the number of bands.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be divided.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: A stacked array where each row is a subvector of the signature.\n",
    "        \"\"\"\n",
    "        l = len(signature)\n",
    "        assert l % self.b == 0\n",
    "        r = int(l / self.b)\n",
    "        subvecs = []\n",
    "        for i in range(0, l, r):\n",
    "            subvecs.append(signature[i:i+r])\n",
    "        return np.stack(subvecs)\n",
    "\n",
    "    def add_hash(self, signature: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds a signature to the appropriate LSH buckets based on its subvectors.\n",
    "\n",
    "        Parameters:\n",
    "        - signature (np.ndarray): The MinHash signature to be hashed and added.\n",
    "        \"\"\"\n",
    "        subvecs = self.make_subvecs(signature).astype(str)\n",
    "        for i, subvec in enumerate(subvecs):\n",
    "            subvec = ','.join(subvec)\n",
    "            if subvec not in self.buckets[i].keys():\n",
    "                self.buckets[i][subvec] = []\n",
    "            self.buckets[i][subvec].append(self.counter)\n",
    "        self.counter += 1\n",
    "\n",
    "    def check_candidates(self) -> set:\n",
    "        \"\"\"\n",
    "        Identifies candidate pairs from the LSH buckets that could be potential near duplicates.\n",
    "\n",
    "        Returns:\n",
    "        - set: A set of tuple pairs representing the indices of candidate signatures.\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        for bucket_band in self.buckets:\n",
    "            keys = bucket_band.keys()\n",
    "            for bucket in keys:\n",
    "                hits = bucket_band[bucket]\n",
    "                if len(hits) > 1:\n",
    "                    candidates.extend(combinations(hits, 2))\n",
    "        return set(candidates)\n",
    "\n",
    "# 25 bands gives threshold around 0.7 similarity\n",
    "n_buckets = 25\n",
    "lsh = LSH(n_buckets)\n",
    "\n",
    "for signature in signatures:\n",
    "    lsh.add_hash(signature)\n",
    "\n",
    "candidate_pairs = lsh.check_candidates()\n",
    "\n",
    "# built a list that gives each dataset an index position\n",
    "len_acm = len(df_ACM)\n",
    "index_to_id = []\n",
    "for i in range(len_acm):\n",
    "    record_id = df_ACM[\"id\"].iloc[i]\n",
    "    index_to_id.append((\"ACM\", record_id))\n",
    "for i in range(len(df_DB)):\n",
    "    record_id = df_DB[\"id\"].iloc[i]\n",
    "    index_to_id.append((\"DBLP\", record_id))\n",
    "\n",
    "# keep only the pairs where the two documents come from different sources\n",
    "cross_source_pairs = [(i, j) for (i, j) in candidate_pairs\n",
    "                      if index_source_id[i][0] != index_source_id[j][0]]\n",
    "\n",
    "#  Score them using MinHash signature similarity\n",
    "scored = []\n",
    "for i, j in cross_source_pairs:\n",
    "    sim = compute_signature_similarity(signatures[i], signatures[j])\n",
    "    scored.append((sim, i, j))\n",
    "\n",
    "#  Sort by similarity and take top 2224\n",
    "scored.sort(reverse=True, key=lambda x: x[0])\n",
    "top_n = scored[:2224]\n",
    "\n",
    "#  create set of predicted pairs\n",
    "pred_pairs = set()\n",
    "for sim, i, j in top_n:\n",
    "    id_acm = str(index_to_id[i][1])\n",
    "    id_dblp = str(index_to_id[j][1])\n",
    "    pred_pairs.add((id_dblp, id_acm))\n",
    "\n",
    "true_pairs = set(zip(perfect_mapping_df[\"idDBLP\"].astype(str),\n",
    "                     perfect_mapping_df[\"idACM\"].astype(str)))\n",
    "\n",
    "#  Compute precision\n",
    "tp = len(pred_pairs.intersection(true_pairs))\n",
    "fp = len(pred_pairs) - tp\n",
    "precision = tp / len(pred_pairs) if pred_pairs else 0.0\n",
    "\n",
    "\n",
    "print(\"precision =\", round(precision, 4))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n---\")\n",
    "print(\"running time:\", round(elapsed_time, 2), \"seconds\")"
   ],
   "id": "e94696ac3334711c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.8251\n",
      "\n",
      "---\n",
      "running time: 2.53 seconds\n"
     ]
    }
   ],
   "execution_count": 132
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
