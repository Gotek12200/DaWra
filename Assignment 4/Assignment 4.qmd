---
title: "Text clustering"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
execute:
  warning: false
  message: false
  echo: true
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false


library(NMF)
library(RColorBrewer)
library(Rtsne)
library(topicmodels)
library(text2vec)
library(wordcloud)
library(magrittr)
library(tm)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stringr)
library(stopwords)
library(caTools)
library(RcppML)
library(scales)
library(cluster)      
library(proxy)
library(clusterSim)
library(mclust)       
library(tibble) 
library(dplyr)
library(text2vec)
```

```{r}
#| label: data loading

data("movie_review")
movie_review <- as_tibble(movie_review)
```

```{r}
glimpse(movie_review) #take a quick glance at data
summary(movie_review) 
sapply(movie_review, class) #identify classes of dataset variables for sentiment analysis
table(movie_review$sentiment) #see the distribution of negative and positive reviews
```

# Data description
Based on the dataset "movie_review" embedded in text2vec package, we see the selection of positive and negative reviews of movie viewers, relatively evenly distributed in the dataset (2483 negative and 2517 positive reviews) denoted with score 1 if the IMDB rating was >= 7 or 0 if rating is <5 accordingly.



Describe the data and use a visualization to support your story. (approx. one or two paragraphs

```{r}
#| label: eda visualization
#| warning: false

# your R code to generate the plot here
movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))

```

# Text pre-processing
### Train test split
Before starting text pre-processing steps, we first need to split the train dataset to a sample used for training, a sample used for validation during training and a test sample to check performance after the training is done. We have decided to split the train data 80:10:10 (training: validation: testing)


```{r}
split_movie_dataset <- function (movie_review, sentiment = "sentiment"){

      # set seed for reproducibility
      set.seed(123)

      #first split the full data set in a 80/20 test train set then split the test val data 50/50 so they both become 10% of the full dataset
      train_split <- sample.split(movie_review$sentiment, SplitRatio = 0.8)
      val_split <- sample.split(movie_review$sentiment[!train_split], SplitRatio = 0.5)

      # save the row indices that are part of the set
      train_ids <- which(train_split)
      temp_ids <- which(!train_split)
      val_ids <- temp_ids[val_split]
      test_ids <- temp_ids[!val_split]

      # return final datasets
      return(list(
          train_set = movie_review[train_ids, ],
          test_set = movie_review[test_ids, ],
          val_set = movie_review[val_ids, ]
      ))
}

data_splits <- split_movie_dataset(movie_review = movie_review)
train_set <- data_splits$train_set
val_set <- data_splits$val_set
test_set <- data_splits$test_set
```
For sentimental analysis enhancement we will further on clean the text from review column based on pre-processing methods @selectedbyourgroup.
But first, we will change sentiment column to factor class:

```{r}
train_set$sentiment <- as.factor(train_set$sentiment)
sapply(train_set, class) #checking the factor change
```
Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.

```{r}
train_set <- train_set %>% 
  filter(!is.na(review)) %>%
  distinct(review, .keep_all = T)
```
Now, we will lowercase all the letters in the texts:
```{r}
clean_train_set <- train_set %>%
  mutate(review = review %>%
      str_to_lower() %>% # induce lowercase
  str_squish()) #remove extra space
```

Next, we will tokenize review column:

```{r}
clean_train_set <- clean_train_set |> 
  unnest_tokens(word, review)
```

We then received 967628 observations from 3997 reviews; now let's see the most common words from our reviews (we use ggplot function):

```{r}
clean_train_set %>%
  mutate(word = as.character(word)) %>%
  dplyr::count(word) %>%              # force dplyr version
  arrange(desc(n)) %>%                # manually sort
  head(30) %>%
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col(fill = "lightblue") +
  labs(x = "Frequency", y = "Words") +
  theme_classic()
```
We see that many of the top words can be considered "stop words" since they bring no significant importance to review meanings, let's remove stop words(except for not because then we can imply window context words to identify negative or positive meanings of bigger phrases:

```{r}
stop_words <- c(
  "the", "and", "a", "of", "to", "is", "it", "in", "this", "that", "was",
  "as", "with", "for", "movie", "but", "film", "you", "he", "his", "are", "have", "be", "one"
)
stop_words <- tibble(word=stop_words)
  clean_train <- clean_train_set %>%
    anti_join(stop_words, by = "word")
```
Eventually, we are left with 663508 observations. Let's see our top words now:
```{r}
clean_train %>%
  mutate(word = as.character(word)) %>%      # ensure character (not a list)
  dplyr::count(word, name = "n") %>%         # count words
  arrange(desc(n)) %>%                       # sort by frequency
  slice_head(n = 30) %>%                     # keep top 30
  ggplot(aes(x = n, y = fct_reorder(word, n))) +
  geom_col(fill = "lightblue") +
  labs(x = "Frequency", y = "Words") +
  theme_classic()
```
There are still some stop words left, and removing them all manually might be time-consuming. Let's instead use the dataset smart_stopwords from library(stopwords) and modify it with those words that we want to use in the analysis(for example, leaving negative particles to see the negative review sensitivity). We will also add words to the "stoplist" that don't make sense for our analysis or neutral movie words that don't have sentimental signal:

```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
  filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without")) %>%
  rbind(tibble(word = c("br", "made", "make", "movie", "film", "films", "movies", "story", "plot",
  "character", "characters", "people", "show", "watch",
  "life", "scene", "end", "man", "time")))
#removing stop words
  clean_train <- clean_train_set %>%
    anti_join(df_stopwords, by = "word")
```

In the end, we received 384141 observations to work with. 
```{r}
# if 'word' is a list (e.g., each cell has a length-1 character), flatten it safely
clean_train <- clean_train %>%
  mutate(word = if (is.list(word)) purrr::map_chr(word, 1) else as.character(word))

clean_train %>%
  dplyr::count(word, name = "n") %>%   # force dplyr's count
  arrange(desc(n)) %>%
  slice_head(n = 30) %>%
  ggplot(aes(x = n, y = fct_reorder(word, n))) +
  geom_col(fill = "lightblue") +
  labs(x = "Frequency", y = "Words") +
  theme_classic()
```


```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
      filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without")) %>%
      rbind(tibble(word = c("br", "made", "make", "movie", "film", "films", "movies", "story", "plot",
      "character", "characters", "people", "show", "watch",
      "life", "scene", "end", "man", "time")))

clean_docs_data <- function (data, df_stopwords){

    cleaned_docs <- data %>%
        # First, we will change sentiment column to factor class:
        mutate(sentiment = as.factor(sentiment)) %>%

        # Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.
        dplyr::filter(!is.na(review)) %>%
        distinct(review, .keep_all = T) %>%

        # We will lowercase all the letters in the texts:
        mutate(review = review %>%
          str_to_lower() %>% # induce lowercase
          str_squish()) %>%  # remove extra space

        # Next, we will tokenize review column:
        unnest_tokens(word, review) %>%

        # Removing stop words
        anti_join(df_stopwords, by = "word") %>%

        # Recombining words into reviews for the DTM
        group_by(id) %>%
        summarize(review = paste(word, collapse = " ")) %>%
        ungroup()

    return(cleaned_docs)

}

train_docs <- clean_docs_data(data = data_splits$train_set, df_stopwords)
val_docs <- clean_docs_data(data = data_splits$val_set, df_stopwords)
test_docs <- clean_docs_data(data = data_splits$test_set, df_stopwords)

```


Document_term_matrix:

```{r}

build_dtm_parts <- function(docs, term_count_min = 5){
    # create index-tokens
    it <- itoken(docs$review, progressbar = FALSE)

    # collecting unique terms
    vocab <- create_vocabulary(it)

    # filtering infrequent terms
    vocab <- prune_vocabulary(vocab, term_count_min)

    # creating the vectorizer
    vectorizer <- vocab_vectorizer(vocab)

    # creating document term matrix
    dtm <- create_dtm(it, vectorizer)

    return(list(
            dtm = dtm,
            vocab = vocab,
            vectorizer = vectorizer
            ))
}

dtm_parts <- build_dtm_parts(train_docs)
dtm <- dtm_parts$dtm
vocab <- dtm_parts$train
vectorizer <- dtm_parts$vectorizer
dtm_matrix <- as.matrix(dtm)

```

# Text representaion

Briefly describe your text representation method. (approx. one or two paragraphs)

# Text clustering
## K-MEANS CLUSTERING
```{r}

k_means_clustering <- function (dtm_matrix, k, seed = 123){

    set.seed(seed)
    km <- kmeans(dtm_matrix, centers = k)

    return(list(
    km = km,
    clusters = km$clusters
    ))
}
kmeans_k5 <- k_means_clustering(
    dtm_matrix = dtm_matrix,
    k = 5
    )
kmeans_k5 <- k_means_clustering(
    dtm_matrix = dtm_matrix,
    k = 10
    )

```
## LDA
```{r}

lda_model <- function(dtm, k, seed = 123) {
  row_sums <- Matrix::rowSums(dtm)
  non_empty_rows <- which(row_sums > 0)
  dtm_filtered <- dtm[non_empty_rows, ]
  set.seed(seed)
  lda_model <- LDA(
    dtm_filtered,
    k = k,
    method = "Gibbs",
    control = list(seed = seed)
  )
  return(lda_model)
}



lda_model_k5 <- lda_model(
    dtm,
    k = 5
    )
lda_model_k10 <- lda_model(
    dtm,
    k = 10
    )

```

## LSA
```{r}

lsa_kmeans <- function(dtm, k, n_topics, seed = 123, nstart = 25){

    lsa_model <- LSA$new(n_topics = n_topics)
    doc_vecs_lsa <- lsa_model$fit_transform(dtm)

    set.seed(seed)
    kmeans <- kmeans(
        as.matrix(doc_vecs_lsa),
        centers = k,
        nstart = nstart
    )
    return(list(
        kmeans = kmeans,
        clusters = kmeans$clusters,
        lsa_matrix = doc_vecs_lsa
    ))
}

lsa_kmeans_k5 <- lsa_kmeans(
  dtm = dtm_matrix,
  k = 5,
  n_topics = 100
)

lsa_kmeans_k10 <- lsa_kmeans(
  dtm = dtm_matrix,
  k = 10,
  n_topics = 100
)


```
## NMF
```{r}
# Number of clusters
k <- 5

# Running the model
nmf_res <- nmf(dtm_matrix, k = k, seed = 123)

# Document topic matrix
W <- nmf_res$w

# Topic-term matrix
H <- nmf_res$h

# Inspect top words per topic
top_n_words <- 10
terms <- colnames(dtm_matrix)

for (i in 1:k) {
  topic_terms <- terms[order(as.numeric(H[i, ]), decreasing = TRUE)[1:top_n_words]]
  cat(paste0("Topic ", i, ": ", paste(topic_terms, collapse = ", "), "\n"))
}

# Assigning each document to the topic with the highest weight
clusters <- apply(W, 1, which.max)
train_docs$nmf_cluster <- clusters

# Count of documents per cluster
table(train_docs$nmf_cluster)

# Optional: visualize top words per cluster using wordcloud
library(wordcloud)

for (i in 1:k) {
  cluster_words <- unlist(strsplit(train_docs$review[train_docs$nmf_cluster == i], " "))
  cluster_words <- cluster_words[cluster_words %in% terms]  # keep only vocab terms
  wordcloud(cluster_words, max.words = 50, colors = brewer.pal(8, "Dark2"))
}
```

### Visualization
```{r}
library(ggplot2)

plot_df <- data.frame(
  cluster = as.factor(clusters)
)
print(ggplot(plot_df, aes(x = cluster, fill = cluster)) +
  geom_bar() +
  ggtitle("K-means") +
  theme(plot.title = element_text(hjust = 0.5)))


```
Briefly describe which models you compare to perform clustering. (approx. two or three paragraphs)

# Evaluation & model comparison

To compare the clustering methods, we used four metrics: Silhouette, Davies–Bouldin Index (DB Index), Adjusted Rand Index (ARI), and Purity. Silhouette and DB Index are internal metrics, which tell us how well the data points fit within their assigned clusters and how clearly the clusters are separated. In contrast, ARI and Purity are external metrics, which compare the clustering results to the actual sentiment labels. ARI checks how closely the clusters match the real labels (adjusted for random chance), while Purity measures how many reviews in each cluster share the same sentiment.

We chose these metrics because each one captures a different aspect of clustering quality. The Silhouette score shows how well-separated and consistent the clusters are, making it a good overall measure of cluster structure. The DB Index was added as a second internal metric because it focuses on how compact and distinct the clusters are, helping confirm the Silhouette results. The ARI was chosen since we already have true sentiment labels, and it tells us how closely the clusters match those labels beyond random chance. Finally, we used Purity because it’s easy to interpret, it shows how clearly each cluster corresponds to one main sentiment (positive or negative). Together, these four metrics give a balanced view of both the clustering performance and its real-world meaning.

```{r}
# purity: for each cluster, take the most common label and average over all docs
purity <- function(true, pred){
  tab <- table(true, pred)
  sum(apply(tab, 2, max)) / length(true)
}

# mean silhouette across docs (silhouette() wants a distance matrix)
sil_mean <- function(cluster_labels, D){
  s <- silhouette(cluster_labels, D)
  mean(s[, 3])  # column 3 is the actual silhouette value
}


# note: everything downstream assumes train_docs order == dtm row order
if (!("id" %in% names(train_set))) train_set <- train_set %>% mutate(id = dplyr::row_number())

train_tmp <- dplyr::select(train_set, id, sentiment)
train_tmp <- dplyr::filter(train_tmp, id %in% train_docs$id)
train_tmp <- dplyr::arrange(train_tmp, match(id, train_docs$id))
labels <- dplyr::pull(train_tmp, sentiment)

# sanity check: labels length should match number of rows in dtm
stopifnot(length(labels) == nrow(dtm_matrix))


# K-Means: we ran on counts (dtm_matrix), so use cosine on that
D_counts <- proxy::dist(dtm_matrix, method = "cosine")

# LSA: use the reduced doc vectors we saved
D_lsa5  <- proxy::dist(as.matrix(lsa_kmeans_k5$lsa_matrix),  method = "cosine")
D_lsa10 <- proxy::dist(as.matrix(lsa_kmeans_k10$lsa_matrix), method = "cosine")

# LDA: distances in topic-proportion space (more fair than raw words)
non_empty_rows <- which(Matrix::rowSums(dtm) > 0)  # LDA dropped empty docs
labels_lda <- labels[non_empty_rows]               # line up labels to those docs
doc_topic5  <- posterior(lda_model_k5)$topics
doc_topic10 <- posterior(lda_model_k10)$topics
D_lda5  <- proxy::dist(doc_topic5,  method = "cosine")
D_lda10 <- proxy::dist(doc_topic10, method = "cosine")
stopifnot(length(labels_lda) == nrow(doc_topic5),
          length(labels_lda) == nrow(doc_topic10))

# NMF: distances in W (doc-topic) space as well
D_nmf5 <- proxy::dist(W, method = "cosine")


# K-Means: note our helper stored the model in $km; labels are $km$cluster
# (we only have k=10 here because the k=5 object got overwritten earlier)
km10_clusters <- kmeans_k5$km$cluster

# LSA + K-Means: clusters are inside the returned kmeans object
lsa5_clusters  <- lsa_kmeans_k5$kmeans$cluster
lsa10_clusters <- lsa_kmeans_k10$kmeans$cluster

# LDA: MAP topic per doc = the argmax over topic probabilities
lda5_clusters  <- max.col(doc_topic5,  ties.method = "first")
lda10_clusters <- max.col(doc_topic10, ties.method = "first")

# NMF (k=5): we saved the hard cluster per doc earlier
nmf5_clusters <- train_docs$nmf_cluster


# higher silhouette is better, lower DB index is better
sil_km10 <- sil_mean(km10_clusters, D_counts)
db_km10  <- index.DB(dtm_matrix, km10_clusters, centrotypes = "centroids")$DB

sil_lsa5  <- sil_mean(lsa5_clusters,  D_lsa5)
sil_lsa10 <- sil_mean(lsa10_clusters, D_lsa10)
db_lsa5   <- index.DB(as.matrix(lsa_kmeans_k5$lsa_matrix),  lsa5_clusters,  centrotypes = "centroids")$DB
db_lsa10  <- index.DB(as.matrix(lsa_kmeans_k10$lsa_matrix), lsa10_clusters, centrotypes = "centroids")$DB

sil_lda5  <- sil_mean(lda5_clusters,  D_lda5)
sil_lda10 <- sil_mean(lda10_clusters, D_lda10)
db_lda5   <- index.DB(doc_topic5,  lda5_clusters,  centrotypes = "centroids")$DB
db_lda10  <- index.DB(doc_topic10, lda10_clusters, centrotypes = "centroids")$DB

sil_nmf5 <- sil_mean(nmf5_clusters, D_nmf5)
db_nmf5  <- index.DB(W, nmf5_clusters, centrotypes = "centroids")$DB


# ARI is “how close are we to the real labels, beyond random chance?”
# Purity is “within each cluster, do most reviews share the same label?”
ari_km10 <- adjustedRandIndex(labels, km10_clusters)
pur_km10 <- purity(labels, km10_clusters)

ari_lsa5  <- adjustedRandIndex(labels, lsa5_clusters)
ari_lsa10 <- adjustedRandIndex(labels, lsa10_clusters)
pur_lsa5  <- purity(labels, lsa5_clusters)
pur_lsa10 <- purity(labels, lsa10_clusters)

ari_lda5  <- adjustedRandIndex(labels_lda, lda5_clusters)
ari_lda10 <- adjustedRandIndex(labels_lda, lda10_clusters)
pur_lda5  <- purity(labels_lda, lda5_clusters)
pur_lda10 <- purity(labels_lda, lda10_clusters)

ari_nmf5 <- adjustedRandIndex(labels, nmf5_clusters)
pur_nmf5 <- purity(labels, nmf5_clusters)


results <- tibble::tibble(
  model = c("K-Means (k=10)",
            "LSA + K-Means (k=5)", "LSA + K-Means (k=10)",
            "LDA (k=5)", "LDA (k=10)",
            "NMF (k=5)"),
  silhouette = c(sil_km10,
                 sil_lsa5, sil_lsa10,
                 sil_lda5, sil_lda10,
                 sil_nmf5),
  db_index   = c(db_km10,
                 db_lsa5, db_lsa10,
                 db_lda5, db_lda10,
                 db_nmf5),
  ARI        = c(ari_km10,
                 ari_lsa5, ari_lsa10,
                 ari_lda5, ari_lda10,
                 ari_nmf5),
  purity     = c(pur_km10,
                 pur_lsa5, pur_lsa10,
                 pur_lda5, pur_lda10,
                 pur_nmf5)
) %>% arrange(desc(ARI))

results

```


```{r}
# order models by ARI (best at top/left)
results_plot <- results %>%
  mutate(model = reorder(model, ARI))

# long format for faceting
results_long <- results_plot %>%
  pivot_longer(cols = c(silhouette, db_index, ARI, purity),
               names_to = "metric", values_to = "value") %>%
  mutate(metric = factor(metric,
                         levels = c("silhouette","db_index","ARI","purity"),
                         labels = c("Silhouette (↑)", "DB Index (↓)", "ARI (↑)", "Purity (↑)")))

ggplot(results_long, aes(x = model, y = value)) +
  geom_col() +
  geom_text(aes(label = round(value, 3)), vjust = -0.3, size = 3) +
  facet_wrap(~ metric, scales = "free_y", ncol = 2) +
  coord_flip() +
  labs(title = "Clustering Evaluation on IMDB Reviews",
       x = NULL, y = NULL,
       caption = "↑ higher is better, ↓ lower is better") +
  theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

```
Among all models, LDA with 5 topics shows the best overall clustering quality.
It achieves strong internal validity (Silhouette = 0.44, DB = 1.16) and the highest external alignment with sentiment labels (ARI = 0.10, Purity = 0.69).
NMF also produces compact clusters (high Silhouette) but with less separation between them (high DB index), suggesting overlapping topics.
LDA with 10 topics still performs reasonably but loses some cohesion due to topic fragmentation.
Traditional K-Means and LSA-based methods perform poorly on text data, as they fail to capture semantic relations between words.
Overall, probabilistic models like LDA and NMF are far more effective for discovering meaningful structure in textual data than distance-based clustering methods.


# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
