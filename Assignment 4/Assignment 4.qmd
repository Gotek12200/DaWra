---
title: "Text clustering"
author:
- Osaro Orebor 1168827
- Máté Csikós-Nagy 4395565
- Fani Profiti 1240390
- Yara Yachnyk 4913329
- Niels Wagenaar 3133998
  date: last-modified
  format:
  html:
  toc: true
  self-contained: true
  code-fold: true
  df-print: kable
  execute:
  warning: false
  message: false
  echo: true
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(NMF)
library(RColorBrewer)
library(Rtsne)
library(topicmodels)
library(text2vec)
library(wordcloud)
library(magrittr)
library(tm)
library(tidyverse)
library(tidytext)
library(stringr)
library(stopwords)
library(caTools)
library(RcppML)
library(scales)
library(cluster)
library(proxy)
library(clusterSim)
library(mclust)
library(tibble)
library(dplyr)
library(proxyC)
library(Silhouette)
library(Matrix)
```

```{r}
#| label: data loading

data("movie_review")
movie_review <- as_tibble(movie_review)
```

```{r}
glimpse(movie_review) #take a quick glance at data
summary(movie_review)
describe(movie_review)
sapply(movie_review, class) #identify classes of dataset variables for sentiment analysis
table(movie_review$sentiment) #see the distribution of negative and positive reviews
```

# Data description
Based on the dataset "movie_review" embedded in text2vec package, we see the selection of positive and negative reviews
of movie viewers, relatively evenly distributed in the dataset (2483 negative and 2517 positive reviews) denoted with
score 1 if the IMDB rating was >= 7 or 0 if rating is <5 accordingly.
The function describe from psych package allows us to see important descriptive statistics characteristics of a dataset,
for example mean, sd, and se 1) the dataset contains 5,000 observations of three variables: id, sentiment and review. 2)
In this dataset *mean* of sentiment is 0.5, indicating that on average half the ratings are positive and half are
negative.
The wordcloud visualization below populates the most common words in the dataset.
```{r}
#| label: eda visualization
#| warning: false

movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))

```

# Text pre-processing

For sentimental analysis enhancement we will further on clean the text from review column based on several
pre-processing methods. first, we converty the sentiment column to a factor and remove any empty or duplicate rows. this
leaves us with a clean dataset. for the text itself we lowercase everything and then tokenize the reviews into
individual words.
when we look at the most common words it is clear there are many stopwords(like "the", "a", "is") that dont have any
sentimental meaning. to remove them all we are using the smart_stopwords list from library(stopwords). we are modifying
this list in two ways, we want to keep negative or other sentimental words so we filter those to remove them from our
list. then we are also adding our own neutral, movie-specific words to the stopword list.

```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
  filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without")) %>%
  rbind(tibble(word = c("br", "made", "make", "movie", "film", "films", "movies", "story", "plot",
  "character", "characters", "people", "show", "watch",
  "life", "scene", "end", "man", "time")))

clean_docs_data <- function (data, df_stopwords){

    cleaned_docs <- data %>%
        # First, we will change sentiment column to factor class
        mutate(sentiment = as.factor(sentiment)) %>%

        # Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.
        dplyr::filter(!is.na(review)) %>%
        distinct(review, .keep_all = T) %>%

        # We will lowercase all the letters in the texts and remove extra space
        mutate(review = review %>%
          str_to_lower() %>%
          str_squish()) %>%

        # Next, we will tokenize review column
        unnest_tokens(word, review) %>%

        # Removing stop words
        anti_join(df_stopwords, by = "word") %>%

        # Recombining words into reviews for the DTM
        group_by(id) %>%
        summarize(review = paste(word, collapse = " ")) %>%
        ungroup()

    return(cleaned_docs)

}
docs <- clean_docs_data(data = movie_review, df_stopwords)


```

# Text representaion
For the text representation method we selected DTM (Document-Term Matrix). In a document term matrix the rows correspond
to the documents in the corpus (in our case the movie reviews) and the columns correspond to the terms in the
documents (in our case the cleaned words of the reviews). The cell values are the frequency of the given word in the
given document.The resulting matrix is typically very sparse, since most words appear only in a few documents.

Before creating the DTM, we also filter out infrequent terms using `term_count_min=5`, which removes words that occur
fewer than 5 times in the entire corpus. This reduces noise and makes the DTM faster to process.
after creating the DTM we filter out all the empty rows to create a filtered sparse matrix which will improve time
complexity and

```{r}

build_dtm_parts <- function(docs, term_count_min = 5){
    # create index-tokens
    it <- itoken(docs$review, progressbar = FALSE)

    # collecting unique terms
    vocab <- create_vocabulary(it)

    # filtering infrequent terms
    vocab <- prune_vocabulary(vocab, term_count_min)

    # creating the vectorizer
    vectorizer <- vocab_vectorizer(vocab)

    # creating document term matrix
    dtm <- create_dtm(it, vectorizer)

    row_sums <- Matrix::rowSums(dtm_full)
    non_empty_rows <- which(row_sums > 0)
    dtm_filtered <- dtm_full[non_empty_rows, ]

    return(list(
            dtm = dtm,
            vocab = vocab,
            vectorizer = vectorizer,
            dtm_filtered = dtm_filtered
            ))
}

dtm_parts <- build_dtm_parts(docs)
dtm <- dtm_parts$dtm
vocab <- dtm_parts$train
vectorizer <- dtm_parts$vectorizer
dtm_filtered <- dtm_parts$dtm_filtered

```

# Text clustering
## K-MEANS CLUSTERING
K- means clustering is an unsupervised machine learning algorithm and shows which data points are similar based to their
distance. The closer together are the data points the more similar they are. The clusters are created by assigning each
data point to the nearest centroid, then iteratively the centroids are calculated again until the clusters stabilize.
```{r}

k_means_clustering <- function (dtm_filtered, k, seed = 123){

    set.seed(seed)
    km <- kmeans(dtm_filtered, centers = k)

    return(list(
    km = km,
    clusters = km$cluster
    ))
}
kmeans_k5 <- k_means_clustering(
    dtm_filtered = dtm_filtered,
    k = 5
    )
kmeans_k10 <- k_means_clustering(
    dtm_filtered = dtm_filtered,
    k = 10
    )

```
## LSA + kmeans
looking at the data we can see that we have a very high dimensionality. kmeans clustering usually performs very bad on a
DTM because of the curse of dimensionality. LSA will solve this problem by dimensionality reduction.
It uses a method called Singular Value Decomposition to reduce the dimensions to, in our case n_topics = 100, "latent
concepts". after we have reduced the data to a dense matrix that sees the underlying semantics of the reviews we should
be able to successfully perform k-means clustering.


```{r}

lsa_kmeans <- function(dtm_filtered, k, n_topics, seed = 123, nstart = 25){

    lsa_model <- LSA$new(n_topics = n_topics)
    doc_vecs_lsa <- lsa_model$fit_transform(dtm_filtered)

    set.seed(seed)
    kmeans <- kmeans(
        as.matrix(doc_vecs_lsa),
        centers = k,
        nstart = nstart
    )
    return(list(
        kmeans = kmeans,
        clusters = kmeans$cluster,
        lsa_matrix = doc_vecs_lsa
    ))
}

lsa_kmeans_k5 <- lsa_kmeans(
  dtm_filtered = dtm_filtered,
  k = 5,
  n_topics = 100
)

lsa_kmeans_k10 <- lsa_kmeans(
  dtm_filtered = dtm_filtered,
  k = 10,
  n_topics = 100
)

```

## LDA
LDA is probabilistic generative model used for topic modelling. it assumes each document in the dtm is a mixture of latent topics, and each topic is a probability distribution over the entire vocabulary. it infers this structure by analyzing word co-occurences. we are applying LDA to our "dtm_filtered" using gibbs sampling, this takes a bit longer but it is more likely to find the best topics in our data because it is a stochastic search and not a deterministic search.

```{r}

lda_model <- function(dtm_filtered, k, seed = 123) {

  set.seed(seed)
  lda_model <- LDA(
    dtm_filtered,
    k = k,
    method = "Gibbs",
    control = list(seed = seed)
  )
  return(lda_model)
}


lda_model_k5 <- lda_model(
    dtm_filtered = dtm_filtered,
    k = 5
    )
lda_model_k10 <- lda_model(
    dtm_filtered = dtm_filtered,
    k = 10
    )


```
## NMF
Non-Negative Matrix Factorization works by decomposing a matrix into two smaller matrices W and H, so that W * H
approximates the original matrix. In our case, W will be the document-topic matrix (rows will be the documents and
columns are the "topics" or clusters), and H will be the topic-term matrix (rows the topics and columns the words). Each
entry in W indicates the strength of association between a document and a topic and each entry in H indicates the
importance of a term for a given topic.

Since all the values are non-negative, NMF produces representations that work with additions (for instance, topics are
combinations of words), so the result is easy to interpret.
```{r}
# Running the models using a method that is able to take a sparse matrix which will also speed up the process
nmf_res_5 <- RcppML::nmf(dtm_filtered, k = 5, seed = 123)
nmf_res_10 <- RcppML::nmf(dtm_filtered, k = 10, seed = 123)

# Document topic matrix
W_5 <- nmf_res_5$w
W_10 <- nmf_res_10$w

# Topic-term matrix
H_5 <- nmf_res_5$h
H_10 <- nmf_res_10$h

# Assigning each document to the topic with the highest weight
nmf5_clusters <- apply(W_5, 1, which.max)
nmf10_clusters <- apply(W_10, 1, which.max)
```
# Evaluation & model comparison

To compare the clustering methods, we used two internal validation metrics: the Silhouette score and the Davies–Bouldin
Index (DB Index). These metrics evaluate how well the data points fit within their assigned clusters and how clearly the
clusters are separated, without relying on any external labels. The Silhouette score measures how similar each document
is to its own cluster compared to other clusters, while the DB Index captures how compact and distinct the clusters are
from one another.

We chose these metrics because they capture a different aspect of clustering quality. The Silhouette score shows how
well-separated and consistent the clusters are, making it a good overall measure of cluster structure. The DB Index was
added as a second internal metric because it focuses on how compact and distinct the clusters are, helping confirm the
Silhouette results.

```{r}

# calculates the average silhouette score for the clusters
# higher values mean that points are well matched to their own cluster
sil_medoid_mean <- function(X, centers) {
  D_obs_centers <- proxy::dist(X, centers, method = "cosine")
  Silhouette(D_obs_centers, method = "medoid")$sil_width |> mean(na.rm = TRUE)
}

# finds the average feature vector (centroid) for each cluster
centroids_from_labels <- function(X, clusters) {
  cs <- sort(unique(clusters))
  do.call(rbind, lapply(cs, function(c) colMeans(X[clusters == c, , drop = FALSE])))
}


# using the models we already trained

# K-Means was done on the DTM (count-based text representation)
X_counts <- dtm_filtered
km5_clusters  <- kmeans_k5$km$cluster     # cluster labels for k = 5
km10_clusters <- kmeans_k10$km$cluster    # cluster labels for k = 10
km5_centers   <- kmeans_k5$km$centers     # cluster centers
km10_centers  <- kmeans_k10$km$centers


# LSA + K-Means: uses LSA document vectors for clustering
X_lsa5  <- as.matrix(lsa_kmeans_k5$lsa_matrix)
X_lsa10 <- as.matrix(lsa_kmeans_k10$lsa_matrix)
lsa5_clusters <- lsa_kmeans_k5$kmeans$cluster
lsa10_clusters <- lsa_kmeans_k10$kmeans$cluster
lsa5_centers  <- lsa_kmeans_k5$kmeans$centers
lsa10_centers <- lsa_kmeans_k10$kmeans$centers


# LDA: get topic proportions per document and assign each doc to the topic with the highest probability
doc_topic5  <- posterior(lda_model_k5)$topics
doc_topic10 <- posterior(lda_model_k10)$topics
lda5_clusters  <- max.col(doc_topic5,  ties.method = "first")
lda10_clusters <- max.col(doc_topic10, ties.method = "first")
lda5_centers   <- centroids_from_labels(doc_topic5,  lda5_clusters)
lda10_centers  <- centroids_from_labels(doc_topic10, lda10_clusters)



# NMF: get cluster assignments and centroids from topic weights
X_nmf5 <- W_5
X_nmf10 <- W_10
nmf5_clusters <- apply(X_nmf5, 1, which.max)
nmf10_clusters <- apply(X_nmf10, 1, which.max)
nmf5_centers  <- centroids_from_labels(X_nmf5, nmf5_clusters)
nmf10_centers <- centroids_from_labels(X_nmf10, nmf10_clusters)


# calculate silhouette scores (higher = better)
sil_km5   <- sil_medoid_mean(X_counts, km5_centers)
sil_km10  <- sil_medoid_mean(X_counts, km10_centers)
sil_lsa5  <- sil_medoid_mean(X_lsa5,  lsa5_centers)
sil_lsa10 <- sil_medoid_mean(X_lsa10, lsa10_centers)
sil_lda5  <- sil_medoid_mean(doc_topic5,  lda5_centers)
sil_lda10 <- sil_medoid_mean(doc_topic10, lda10_centers)
sil_nmf5  <- sil_medoid_mean(X_nmf5, nmf5_centers)
sil_nmf10 <- sil_medoid_mean(X_nmf10, nmf10_centers)


# calculate Davies–Bouldin index (lower = better)
db_km5   <- index.DB(X_counts, km5_clusters,  centrotypes = "centroids")$DB
db_km10  <- index.DB(X_counts, km10_clusters, centrotypes = "centroids")$DB
db_lsa5  <- index.DB(X_lsa5,  lsa5_clusters,  centrotypes = "centroids")$DB
db_lsa10 <- index.DB(X_lsa10, lsa10_clusters, centrotypes = "centroids")$DB
db_lda5  <- index.DB(doc_topic5,  lda5_clusters,  centrotypes = "centroids")$DB
db_lda10 <- index.DB(doc_topic10, lda10_clusters, centrotypes = "centroids")$DB
db_nmf5  <- index.DB(X_nmf5, nmf5_clusters, centrotypes = "centroids")$DB
db_nmf10 <- index.DB(X_nmf10, nmf10_clusters, centrotypes = "centroids")$DB


# put all results into one table to compare models
results <- tibble::tibble(
  model = c("K-Means (k=5)", "K-Means (k=10)",
            "LSA + K-Means (k=5)", "LSA + K-Means (k=10)",
            "LDA (k=5)", "LDA (k=10)",
            "NMF (k=5)", "NMF (k=10)"),
  silhouette = c(sil_km5, sil_km10,
                 sil_lsa5, sil_lsa10,
                 sil_lda5, sil_lda10,
                 sil_nmf5, sil_nmf10),
  db_index   = c(db_km5, db_km10,
                 db_lsa5, db_lsa10,
                 db_lda5, db_lda10,
                 db_nmf5, db_nmf10)
)


results


```
Among all models, LDA with 5 topics gives the best overall clustering results.
It has a strong internal validity (Silhouette = 0.61, DB = 1.20), meaning its clusters are both well-defined and clearly
separated from each other.

NMF (k=10) also performs quite well (Silhouette = 0.56, DB = 9.57). This suggests that while the model captures
meaningful structure, the clusters might still overlap a bit — they’re compact internally but not as well separated.
NMF (k=5), on the other hand, shows weaker separation (DB = 29.06), probably because too few topics make it harder to
capture all the variety in the data.

LDA with 10 topics performs slightly worse than its k=5 version (Silhouette = 0.50, DB = 1.51), likely because
increasing the number of topics causes some splitting of coherent themes.
Meanwhile, K-Means and LSA + K-Means models show very low silhouette scores and high DB values, which means their
clusters aren’t clearly separated and don’t capture the underlying semantics of the text well.

Overall, probabilistic models like LDA and NMF clearly outperform traditional distance-based methods for this kind of
text data, as they can better capture the hidden structure and meaning behind the words.


# Team member contributions

Write down what each team member contributed to the project.

- Osaro Orebor: Evaluated the models, fixed some errors and cleaned up the data.
- Mate Csikos-Nagy: DTM matrix creation, NMF model setup
- Yara Yachnyk: describing data, performing text pre-processing
- Niels Wagenaar:  created function of clean_docs_data, build_dtm_parts, LSA and LDA clustering. finalized the whole
  report.
