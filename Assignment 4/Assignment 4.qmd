---
title: "Text clustering"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
execute:
  warning: false
  message: false
  echo: true
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

library(text2vec)
library(wordcloud)
library(magrittr)
library(tm)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stringr)
library(stopwords)

# additional packages here
```

```{r}
#| label: data loading

data("movie_review")

# your R code to load the data her
```

```{r}
glimpse(movie_review) #take a quick glance at data
summary(movie_review) 
sapply(movie_review, class) #identify classes of dataset variables for sentiment analysis
table(movie_review$sentiment) #see the distribution of negative and positive reviews
```

# Data description
Based on the dataset "movie_review" embedded in text2vec package, we see the selection of positive and negative reviews of movie viewers, relatively evenly distributed in the dataset (2483 negative and 2517 positive reviews) denoted with score 1 if the IMDB rating was >= 7 or 0 if rating is <5 accordingly.



Describe the data and use a visualization to support your story. (approx. one or two paragraphs

```{r}
#| label: eda visualization
#| warning: false

# your R code to generate the plot here
movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))

```

# Text pre-processing

For better analysis we will further on clean the text from review column based on selected pre-processing methods.
But first, we will change sentiment column to factor class:

```{r}
movie_review$sentiment <- as.factor(movie_review$sentiment)
sapply(movie_review, class) #checking the factor change
```
Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.

```{r}
movie_review <- movie_review %>% 
  filter(!is.na(review)) %>%
  distinct(review, .keep_all = T)
```
Now, we will lowercase all the letters in the texts and remove punctuation:
```{r}
clean_movie_review <- movie_review %>%
  mutate(review = review %>%
      str_to_lower() %>% # induce lowercase
  str_replace_all("[[:punct:]]", " ") %>% #remove punctuation
  str_squish()) #remove extra space
```

Next, we will tokenize review column:

```{r}
clean_movie_review <- clean_movie_review |> 
  unnest_tokens(word, review)
```

We then received 1229424 observations from 4996 reviews; now let's see the most common words from our reviews (we use ggplot function):

```{r}
clean_movie_review |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```
We see that many of the top words can be considered "stop words" since they bring no significant importance to review meanings, let's remove stop words(except for not because then we can imply useful combinations of words to identify negative or positive meaning:

```{r}
stop_words <- c(
  "the", "and", "a", "of", "to", "is", "it", "in", "this", "that", "was",
  "as", "with", "for", "movie", "but", "film", "you", "he", "his", "are", "have", "be", "one"
)
stop_words <- tibble(word=stop_words)
  clean_movie <- clean_movie_review %>%
    anti_join(stop_words, by = "word")
```
Eventually, we are left with 843586 observations. Let's see our top words now:
```{r}
clean_movie |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```
There are still some stop words left, and removing them all manually might be time-consuming. Let's instead use the dataset smart_stopwords from library(stopwords) and modify it with those words that we want to use in the analysis(for example, leaving negative particles to see the negative review sensitivity).

```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
  filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", wish", "without"))
#removing stop words
  clean_movie <- clean_movie_review %>%
    anti_join(stop_words, by = "word")
```
```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <-stop_words |>
  filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without"))
#removing stop words
  clean_movie <- clean_movie_review %>%
    anti_join(stop_words, by = "word")
  
```
In the end, we received 521346 observations to work with. 
```{r}
clean_movie |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```

# Text representaion

Briefly describe your text representation method. (approx. one or two paragraphs)

# Text clustering

Briefly describe which models you compare to perform clustering. (approx. two or three paragraphs)

# Evaluation & model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)


```{r}
#| label: table example
data.frame(
  model       = c("clustering model 1", "clustering model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```


# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
