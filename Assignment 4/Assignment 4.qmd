---
title: "Text clustering"
author: 
  - Osaro Orebor 1168827
  - Máté Csikós-Nagy 4395565
  - Fani Profiti 1240390
  - Yara Yachnyk 4913329
  - Niels Wagenaar 3133998
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
execute:
  warning: false
  message: false
  echo: true
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false


library(Rtsne)
library(topicmodels)
library(text2vec)
library(wordcloud)
library(magrittr)
library(tm)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stringr)
library(stopwords)
library(caTools)
library(RcppML)
```

```{r}
#| label: data loading

data("movie_review")
movie_review <- as_tibble(movie_review)
```

```{r}
glimpse(movie_review) #take a quick glance at data
summary(movie_review) 
sapply(movie_review, class) #identify classes of dataset variables for sentiment analysis
table(movie_review$sentiment) #see the distribution of negative and positive reviews
```

# Data description
Based on the dataset "movie_review" embedded in text2vec package, we see the selection of positive and negative reviews of movie viewers, relatively evenly distributed in the dataset (2483 negative and 2517 positive reviews) denoted with score 1 if the IMDB rating was >= 7 or 0 if rating is <5 accordingly.



Describe the data and use a visualization to support your story. (approx. one or two paragraphs

```{r}
#| label: eda visualization
#| warning: false

# your R code to generate the plot here
movie_review %$% wordcloud(review, 
                           min.freq = 10, 
                           max.words = 50, 
                           random.order = FALSE,
                           colors = brewer.pal(8, "Dark2"))

```

# Text pre-processing
### Train test split
Before starting text pre-processing steps, we first need to split the train dataset to a sample used for training, a sample used for validation during training and a test sample to check performance after the training is done. We have decided to split the train data 80:10:10 (training: validation: testing)


```{r}
split_movie_dataset <- function (movie_review, sentiment = "sentiment"){

      # set seed for reproducibility
      set.seed(123)

      #first split the full data set in a 80/20 test train set then split the test val data 50/50 so they both become 10% of the full dataset
      train_split <- sample.split(movie_review$sentiment, SplitRatio = 0.8)
      val_split <- sample.split(movie_review$sentiment[!train_split], SplitRatio = 0.5)

      # save the row indices that are part of the set
      train_ids <- which(train_split)
      temp_ids <- which(!train_split)
      val_ids <- temp_ids[val_split]
      test_ids <- temp_ids[!val_split]

      # return final datasets
      return(list(
          train_set = movie_review[train_ids, ],
          test_set = movie_review[test_ids, ],
          val_set = movie_review[val_ids, ]
      ))
}

data_splits <- split_movie_dataset(movie_review = movie_review)
train_set <- data_splits$train_set
val_set <- data_splits$val_set
test_set <- data_splits$test_set
```
For sentimental analysis enhancement we will further on clean the text from review column based on pre-processing methods @selectedbyourgroup.
But first, we will change sentiment column to factor class:

```{r}
train_set$sentiment <- as.factor(train_set$sentiment)
sapply(train_set, class) #checking the factor change
```
Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.

```{r}
train_set <- train_set %>% 
  filter(!is.na(review)) %>%
  distinct(review, .keep_all = T)
```
Now, we will lowercase all the letters in the texts:
```{r}
clean_train_set <- train_set %>%
  mutate(review = review %>%
      str_to_lower() %>% # induce lowercase
  str_squish()) #remove extra space
```

Next, we will tokenize review column:

```{r}
clean_train_set <- clean_train_set |> 
  unnest_tokens(word, review)
```

We then received 967628 observations from 3997 reviews; now let's see the most common words from our reviews (we use ggplot function):

```{r}
clean_train_set |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```
We see that many of the top words can be considered "stop words" since they bring no significant importance to review meanings, let's remove stop words(except for not because then we can imply window context words to identify negative or positive meanings of bigger phrases:

```{r}
stop_words <- c(
  "the", "and", "a", "of", "to", "is", "it", "in", "this", "that", "was",
  "as", "with", "for", "movie", "but", "film", "you", "he", "his", "are", "have", "be", "one"
)
stop_words <- tibble(word=stop_words)
  clean_train <- clean_train_set %>%
    anti_join(stop_words, by = "word")
```
Eventually, we are left with 663508 observations. Let's see our top words now:
```{r}
clean_train |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```
There are still some stop words left, and removing them all manually might be time-consuming. Let's instead use the dataset smart_stopwords from library(stopwords) and modify it with those words that we want to use in the analysis(for example, leaving negative particles to see the negative review sensitivity). We will also add words to the "stoplist" that don't make sense for our analysis or neutral movie words that don't have sentimental signal:

```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
  filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without")) %>%
  rbind(tibble(word = c("br", "made", "make", "movie", "film", "films", "movies", "story", "plot",
  "character", "characters", "people", "show", "watch",
  "life", "scene", "end", "man", "time")))
#removing stop words
  clean_train <- clean_train_set %>%
    anti_join(df_stopwords, by = "word")
```

In the end, we received 384141 observations to work with. 
```{r}
clean_train |> 
  # count the frequency of each word
  count(word) |> 
  # arrange the words by its frequency in descending order
  arrange(desc(n)) |> 
  # select the top 30 most frequent words
  head(30) |> 
  # (reorder words by their frequencies)
  ggplot(aes(x = n, y = reorder(word, n))) + 
  geom_col(fill="lightblue") +
  labs(x = "frequency", y="words") + 
  theme_classic()
```


```{r}
smart_stopwords <- stopwords(source = "smart")
stop_words <- tibble(word=smart_stopwords)
df_stopwords <- stop_words %>%
      filter(!word %in% c("neither", "never", "no", "nobody", "non", "none", "not", "nothing", "nowhere", "awfully", "best", "better", "cannot", "least", "less", "like", "little", "serious", "thanks", "welcome", "want", "willing", "will", "wish", "without")) %>%
      rbind(tibble(word = c("br", "made", "make", "movie", "film", "films", "movies", "story", "plot",
      "character", "characters", "people", "show", "watch",
      "life", "scene", "end", "man", "time")))

clean_docs_data <- function (data, df_stopwords){

    cleaned_docs <- data %>%
        # First, we will change sentiment column to factor class:
        mutate(sentiment = as.factor(sentiment)) %>%

        # Next, we will remove empty rows with no text or duplicates, which leaves us with 4996 observations.
        dplyr::filter(!is.na(review)) %>%
        distinct(review, .keep_all = T) %>%

        # We will lowercase all the letters in the texts:
        mutate(review = review %>%
          str_to_lower() %>% # induce lowercase
          str_squish()) %>%  # remove extra space

        # Next, we will tokenize review column:
        unnest_tokens(word, review) %>%

        # Removing stop words
        anti_join(df_stopwords, by = "word") %>%

        # Recombining words into reviews for the DTM
        group_by(id) %>%
        summarize(review = paste(word, collapse = " ")) %>%
        ungroup()

    return(cleaned_docs)

}

train_docs <- clean_docs_data(data = data_splits$train_set, df_stopwords)
val_docs <- clean_docs_data(data = data_splits$val_set, df_stopwords)
test_docs <- clean_docs_data(data = data_splits$test_set, df_stopwords)

```


Document_term_matrix:

```{r}

build_dtm_parts <- function(docs, term_count_min = 5){
    # create index-tokens
    it <- itoken(docs$review, progressbar = FALSE)

    # collecting unique terms
    vocab <- create_vocabulary(it)

    # filtering infrequent terms
    vocab <- prune_vocabulary(vocab, term_count_min)

    # creating the vectorizer
    vectorizer <- vocab_vectorizer(vocab)

    # creating document term matrix
    dtm <- create_dtm(it, vectorizer)

    return(list(
            dtm = dtm,
            vocab = vocab,
            vectorizer = vectorizer
            ))
}

dtm_parts <- build_dtm_parts(train_docs)
dtm <- dtm_parts$dtm
vocab <- dtm_parts$train
vectorizer <- dtm_parts$vectorizer
dtm_matrix <- as.matrix(dtm)

```

# Text representaion

Briefly describe your text representation method. (approx. one or two paragraphs)

# Text clustering
## K-MEANS CLUSTERING
```{r}

k_means_clustering <- function (dtm_matrix, k, seed = 123){

    set.seed(seed)
    km <- kmeans(dtm_matrix, centers = k)

    return(list(
    km = km,
    clusters = km$clusters
    ))
}
kmeans_k5 <- k_means_clustering(
    dtm_matrix = dtm_matrix,
    k = 5
    )
kmeans_k5 <- k_means_clustering(
    dtm_matrix = dtm_matrix,
    k = 10
    )

```
## LDA
```{r}

lda_model <- function(dtm, k, seed = 123) {

    # find sum of words per row
    row_sums <- rowSums(dtm)
    # keep the indices of the non-empty rows
    non_empty_rows <- which(row_sums < 0)
    # filter the dtm
    dtm_filtered <- dtm[non_empty_rows, ]
    # set the seed
    set.seed(seed)
    # build the lda model
    lda_model <- LDA(
        dtm_filtered,
        k = k,
        control = seed
    )

    return(lda_model)
}

lda_model_k5 <- lda_model(
    dtm_matrix,
    k = 5
    )
lda_model_k10 <- lda_model(
    dtm_matrix,
    k = 10
    )

```

## LSA
```{r}

lsa_kmeans <- function(dtm, k, n_topics, seed = 123, nstart = 25){

    lsa_model <- LSA$new(n_topics = n_topics)
    doc_vecs_lsa <- lsa_model$fit_transform(dtm)

    set.seed(seed)
    kmeans <- kmeans(
        as.matrix(doc_vecs_lsa),
        centers = k,
        nstart = nstart
    )
    return(list(
        kmeans = kmeans,
        clusters = kmeans$clusters,
        lsa_matrix = doc_vecs_lsa
    ))
}

lsa_kmeans_k5 <- lsa_kmeans(
    dtm = dtm_matrix,
    k = 5,
    dimensions = 100
)

lsa_kmeans_k10 <- lsa_kmeans(
    dtm = dtm_matrix,
    k = 10,
    dimensions = 100
)

```
## NMF
```{r}
# Number of clusters
k <- 5

# Running the model
nmf_res <- nmf(dtm_matrix, k = k, seed = 123)

# Document topic matrix
W <- nmf_res$w

# Topic-term matrix
H <- nmf_res$h

# Inspect top words per topic
top_n_words <- 10
terms <- colnames(dtm_dense)

for (i in 1:k) {
  topic_terms <- terms[order(as.numeric(H[i, ]), decreasing = TRUE)[1:top_n_words]]
  cat(paste0("Topic ", i, ": ", paste(topic_terms, collapse = ", "), "\n"))
}

# Assigning each document to the topic with the highest weight
clusters <- apply(W, 1, which.max)
train_docs$nmf_cluster <- clusters

# Count of documents per cluster
table(train_docs$nmf_cluster)

# Optional: visualize top words per cluster using wordcloud
library(wordcloud)

for (i in 1:k) {
  cluster_words <- unlist(strsplit(train_docs$review[train_docs$nmf_cluster == i], " "))
  cluster_words <- cluster_words[cluster_words %in% terms]  # keep only vocab terms
  wordcloud(cluster_words, max.words = 50, colors = brewer.pal(8, "Dark2"))
}
```

### Visualization
```{r}
library(ggplot2)

plot_df <- data.frame(
  cluster = as.factor(clusters)
)
print(ggplot(plot_df, aes(x = cluster, fill = cluster)) +
  geom_bar() +
  ggtitle("K-means") +
  theme(plot.title = element_text(hjust = 0.5)))


```
Briefly describe which models you compare to perform clustering. (approx. two or three paragraphs)

# Evaluation & model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)


```{r}
#| label: table example
data.frame(
  model       = c("clustering model 1", "clustering model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```



# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
